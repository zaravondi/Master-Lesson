{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "from collections import Counter\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Sequential or session-based recommendation')\n",
    "parser.add_argument('--model', type=str, default='fossil', help='model: fossil/fpmc. (default: fossil)')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size (default: 128)')\n",
    "parser.add_argument('--seq_len', type=int, default=20, help='max sequence length (default: 20)')\n",
    "parser.add_argument('--l2_reg', type=float, default=0.0, help='regularization scale (default: 0.0)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='initial learning rate for Adam (default: 0.01)')\n",
    "parser.add_argument('--lr_decay', type=float, default=0.5, help='learning rate decay rate (default: 0.5)')\n",
    "parser.add_argument('--emsize', type=int, default=100, help='dimension of item embedding (default: 100)')\n",
    "parser.add_argument('--neg_size', type=int, default=1, help='size of negative samples (default: 1)')\n",
    "parser.add_argument('--worker', type=int, default=10, help='number of sampling workers (default: 10)')\n",
    "parser.add_argument('--seed', type=int, default=1111, help='random seed (default: 1111)')\n",
    "parser.add_argument('--data', type=str, default='gowalla', help='data set name (default: gowalla)')\n",
    "parser.add_argument('--log_interval', type=int, default=1e2, help='log interval (default: 1e2)')\n",
    "parser.add_argument('--eval_interval', type=int, default=1e3, help='eval/test interval (default: 1e3)')\n",
    "parser.add_argument('--optim', type=str, default='adam', help='optimizer: sgd/adam (default: adam)')\n",
    "parser.add_argument('--warm_up', type=int, default=0, help='warm up step (default: 0)')\n",
    "# ****************************** unique arguments for FOSSIL *******************************************************\n",
    "parser.add_argument('--alpha', type=float, default=0.2, help='alpha (default: 0.2)')\n",
    "parser.add_argument('--order', type=int, default=1, help='order of Fossil (default: 1)')\n",
    "\n",
    "# ****************************** unique arguments for FPMC *******************************************************\n",
    "# None\n",
    "args = parser.parse_args(args=[])\n",
    "data_path = 'data/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(corpus_item, corpus_user, data, dname, path_to_data):\n",
    "    ret = {}\n",
    "    user_str_ids = data.keys()\n",
    "    for u in user_str_ids:\n",
    "        u_int_id = corpus_user.dict.item2idx[u]\n",
    "        i_int_ids = []\n",
    "        item_str_ids = data[u]\n",
    "        for i in item_str_ids:\n",
    "            i_int_ids.append(corpus_item.dict.item2idx[i])\n",
    "        ret[u_int_id] = i_int_ids\n",
    "    with open(path_to_data + dname + '.json', 'w') as fp:\n",
    "        json.dump(ret, fp)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.item2idx = {}\n",
    "        self.idx2item = []\n",
    "        self.counter = Counter()\n",
    "\n",
    "    def add_item(self, item):\n",
    "        self.counter[item] +=1\n",
    "\n",
    "    def prep_dict(self):\n",
    "        for item in self.counter:\n",
    "            if item not in self.item2idx:\n",
    "                self.idx2item.append(item)\n",
    "                self.item2idx[item] = len(self.idx2item)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, ItemId):\n",
    "        self.dict = Dictionary()\n",
    "        for item in ItemId:\n",
    "            self.dict.add_item(item)\n",
    "        self.dict.prep_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_eval_test(data, batch_size, max_test_len=100):\n",
    "    if batch_size < 2:\n",
    "        batch_size = 2\n",
    "    uids = data.keys()\n",
    "    all_u = []\n",
    "    all_inp = []\n",
    "    all_pos = []\n",
    "    for u in uids:\n",
    "        #all_u.append(int(u))\n",
    "        #cur_u = []\n",
    "        itemids = data[u]\n",
    "        nb_test = min(max_test_len, len(itemids)) - 1\n",
    "        all_u.extend([int(u)] * nb_test)\n",
    "        for i in range(1, nb_test+1):\n",
    "            pos = itemids[i]\n",
    "            inp = np.zeros([max_test_len], dtype=np.int32)\n",
    "            start = max_test_len - i\n",
    "            len_of_item = i\n",
    "            inp[start:] = itemids[i-len_of_item: i]\n",
    "        #inp = np.zeros([max_test_len], dtype=np.int32)\n",
    "        #pos = np.zeros([max_test_len], dtype=np.int32)\n",
    "        #l = min(max_test_len, len(itemids))\n",
    "        #inp[:l] = itemids[:l]\n",
    "        #pos[:l-1] = itemids[1:l]\n",
    "            all_inp.append(inp)\n",
    "            all_pos.append(pos)\n",
    "\n",
    "    num_batches = int(len(all_u) / batch_size)\n",
    "    batches = []\n",
    "    for i in range(num_batches):\n",
    "        batch_u = all_u[i*batch_size: (i+1)*batch_size]\n",
    "        batch_inp = all_inp[i*batch_size: (i+1)*batch_size]\n",
    "        batch_pos = all_pos[i*batch_size: (i+1)*batch_size]\n",
    "        batches.append((batch_u, batch_inp, batch_pos))\n",
    "    if num_batches * batch_size < len(all_u):\n",
    "        batches.append((all_u[num_batches * batch_size:], all_inp[num_batches * batch_size:], all_pos[num_batches * batch_size:]))\n",
    "        \n",
    "    return batches\n",
    "\n",
    "def preprocess_session(dname):\n",
    "    data = pd.read_csv(data_path + dname + '/' + dname + '.tsv', sep='\\t', header=None)\n",
    "    if dname == 'tmall':\n",
    "        data.columns = ['SessionId', 'ItemId', 'Time']\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    session_lengths = data.groupby('SessionId').size()\n",
    "    data = data[np.in1d(data.SessionId, session_lengths[session_lengths>2].index)]\n",
    "    \n",
    "    item_supports = data.groupby('ItemId').size()\n",
    "    data = data[np.in1d(data.ItemId, item_supports[item_supports>=10].index)]\n",
    "    print('Unique items: {}'.format(data.ItemId.nunique()))\n",
    "        \n",
    "    session_lengths = data.groupby('SessionId').size()\n",
    "    print('Average session length: {}'.format(session_lengths.mean()))\n",
    "    data = data[np.in1d(data.SessionId, session_lengths[session_lengths>2].index)]\n",
    "    \n",
    "    session_lengths = data.groupby('SessionId').size()\n",
    "    print('Average session length after removing sessions with less than two event: {}'.format(session_lengths.mean()))\n",
    "    \n",
    "    session_max_times = data.groupby('SessionId').Time.max()\n",
    "    tmax = data.Time.max()\n",
    "    session_train = session_max_times[session_max_times < tmax-86400*2].index\n",
    "    session_test = session_max_times[session_max_times >= tmax-86400*2].index\n",
    "    train = data[np.in1d(data.SessionId, session_train)]\n",
    "    test = data[np.in1d(data.SessionId, session_test)]\n",
    "    test = test[np.in1d(test.ItemId, train.ItemId)]\n",
    "    \n",
    "    tslength = test.groupby('SessionId').size()\n",
    "    test = test[np.in1d(test.SessionId, tslength[tslength>2].index)]\n",
    "\n",
    "    test_session = test.SessionId.unique()\n",
    "    test_session_ = np.random.choice(test_session, int(len(test_session) / 2), replace=False)\n",
    "    test_ = test.loc[test['SessionId'].isin(test_session_)]\n",
    "    val_ = test.loc[~test['SessionId'].isin(test_session_)]\n",
    "    print('Train size: {}'.format(len(train)))\n",
    "    print('Dev size: {}'.format(len(val_)))\n",
    "    print('Test size: {}'.format(len(test_)))\n",
    "\n",
    "    header = columns = ['SessionId', 'ItemId', 'Time']\n",
    "    train.to_csv(data_path + dname + '/' + dname + '_train_tr.txt', sep='\\t', columns=columns, header=header, index=False)\n",
    "    test_.to_csv(data_path + dname + '/' + dname + '_test.txt', sep='\\t',columns=columns, header=header, index=False)\n",
    "    val_.to_csv(data_path + dname + '/' + dname + '_train_valid.txt', sep='\\t', columns=columns, header=header, index=False)\n",
    "\n",
    "\n",
    "def preprocess_sequence():\n",
    "    data = pd.read_csv(data_path + dname + '/' + dname + '.tsv', sep='\\t', header=None)\n",
    "    \n",
    "    data.columns = ['user', 'TimeStr', 'lat', 'long', 'item']\n",
    "    data['Time'] = data.TimeStr.apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%dT%H:%M:%SZ').timestamp())\n",
    "    del(data['lat'])\n",
    "    del(data['long'])\n",
    "    del(data['TimeStr'])\n",
    "   \n",
    "    \n",
    "    event_lengths = data.groupby('user').size()\n",
    "    print('Average check-ins per user: {}'.format(event_lengths.mean()))\n",
    "    data = data[np.in1d(data.user, event_lengths[event_lengths>10].index)]\n",
    "    \n",
    "    item_supports = data.groupby('item').size()\n",
    "    # 50 for delicious, 10 for gowalla\n",
    "    data = data[np.in1d(data.item, item_supports[item_supports>=10].index)]\n",
    "    print('Unique items: {}'.format(data.item.nunique()))\n",
    "    \n",
    "    event_lengths = data.groupby('user').size()\n",
    "    data = data[np.in1d(data.user, event_lengths[event_lengths>=10].index)]\n",
    "    \n",
    "    event_lengths = data.groupby('user').size()\n",
    "    print('Average check-ins per user after removing sessions with one event: {}'.format(event_lengths.mean()))\n",
    "    \n",
    "    tmin = data.Time.min()\n",
    "    tmax = data.Time.max()\n",
    "    pivot = (tmax-tmin) * 0.9 + tmin\n",
    "    train = data.loc[data['Time'] < pivot]\n",
    "    test = data.loc[data['Time'] >= pivot]\n",
    "\n",
    "    tr_event_lengths = train.groupby('user').size()\n",
    "    train = train[np.in1d(train.user, tr_event_lengths[tr_event_lengths>3].index)]\n",
    "    print('Average (train) check-ins per user: {}'.format(tr_event_lengths.mean()))\n",
    "\n",
    "    user_to_predict = train.user.unique()\n",
    "    test = test[test['user'].isin(user_to_predict)]\n",
    "    item_to_predict = train.item.unique()\n",
    "    test = test[test['item'].isin(item_to_predict)]\n",
    "    test_event_lengths = test.groupby('user').size()\n",
    "    test = test[np.in1d(test.user, test_event_lengths[test_event_lengths>3].index)]\n",
    "    print('Average (test) check-ins per user: {}'.format(test_event_lengths.mean()))\n",
    "\n",
    "   \n",
    "    test_user = test.user.unique()\n",
    "    test_user_ = np.random.choice(test_user, int(len(test_user) / 2), replace=False)\n",
    "    test_ = test.loc[test['user'].isin(test_user_)]\n",
    "    val_ = test.loc[~test['user'].isin(test_user_)]\n",
    "    print('Train size: {}'.format(len(train)))\n",
    "    print('Dev size: {}'.format(len(val_)))\n",
    "    print('Test size: {}'.format(len(test_)))\n",
    "\n",
    "  \n",
    "    header = columns = ['user', 'item', 'Time']\n",
    "    header = ['UserId', 'ItemId', 'Time']\n",
    "    train.to_csv(data_path + dname + '/' + dname + '_train_tr.txt', sep='\\t', columns=columns, header=header, index=False)\n",
    "    test_.to_csv(data_path + dname + '/' + dname + '_test.txt', sep='\\t',columns=columns, header=header, index=False)\n",
    "    val_.to_csv(data_path + dname + '/' + dname + '_train_valid.txt', sep='\\t', columns=columns, header=header, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(args):\n",
    "    path_to_data= data_path + args.data + '/'\n",
    "    if not os.path.exists(path_to_data + args.data + '_train_tr.json'):\n",
    "        tr_df = pd.read_csv(path_to_data + args.data + '_train_tr.txt', sep='\\t')\n",
    "        val_df = pd.read_csv(path_to_data + args.data + '_train_valid.txt', sep='\\t')\n",
    "        test_df = pd.read_csv(path_to_data + args.data + '_test.txt', sep='\\t')\n",
    "        corpus_item = Corpus(tr_df['ItemId'])\n",
    "        corpus_user = Corpus(tr_df['UserId'])\n",
    "        np.save(path_to_data + args.data + '_item_dict', np.asarray(corpus_item.dict.idx2item))\n",
    "        np.save(path_to_data + args.data + '_user_dict', np.asarray(corpus_user.dict.idx2item))\n",
    "\n",
    "        tr = tr_df.sort_values(['UserId', 'Time']).groupby('UserId')['ItemId'].apply(list).to_dict()\n",
    "        val = val_df.sort_values(['UserId', 'Time']).groupby('UserId')['ItemId'].apply(list).to_dict()\n",
    "        test = test_df.sort_values(['UserId', 'Time']).groupby('UserId')['ItemId'].apply(list).to_dict()\n",
    "            \n",
    "        _ = prepare_data(corpus_item, corpus_user, tr, args.data + '_train_tr', path_to_data)\n",
    "        _ = prepare_data(corpus_item, corpus_user, val, args.data + '_train_valid',path_to_data)\n",
    "        _ = prepare_data(corpus_item, corpus_user, test, args.data + '_test', path_to_data)\n",
    "\n",
    "    with open(path_to_data + args.data + '_train_tr.json', 'r') as fp:\n",
    "        train_data = json.load(fp)\n",
    "    with open(path_to_data + args.data + '_train_valid.json', 'r') as fp:\n",
    "        val_data = json.load(fp)\n",
    "    with open(path_to_data + args.data + '_test.json', 'r') as fp:\n",
    "        test_data = json.load(fp)\n",
    "\n",
    "    item2idx = np.load(path_to_data + args.data + '_item_dict.npy')\n",
    "    user2idx = np.load(path_to_data + args.data + '_user_dict.npy')\n",
    "    n_items = item2idx.size\n",
    "    n_users = user2idx.size\n",
    "    \n",
    "    return [train_data, val_data, test_data, n_items, n_users]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(args.seed)\n",
    "train_data, val_data, test_data, n_items, n_users = data_generator(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler.py\n",
    "import numpy as np\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def random_neg(pos, n, s):\n",
    "    '''\n",
    "    p: positive one\n",
    "    n: number of items\n",
    "    s: size of samples.\n",
    "    '''\n",
    "    neg = set()\n",
    "    for _ in range(s):\n",
    "        t = np.random.randint(1, n+1)\n",
    "        while t in pos or t in neg:\n",
    "            t = np.random.randint(1, n+1)\n",
    "        neg.add(t)\n",
    "    return list(neg)\n",
    "\n",
    "def sample_function(data, n_items, n_users, batch_size, max_len, neg_size, result_queue, SEED, neg_method='rand'):\n",
    " \n",
    "    num_samples = np.array([len(data[str(u)]) for u in range(1, n_users+1)])\n",
    "    prob_ = num_samples / (1.0 * np.sum(num_samples))\n",
    "    def sample():\n",
    "        \n",
    "        user = np.random.choice(a=range(1,1+n_users), p=prob_)\n",
    "        u = str(user)\n",
    "\n",
    "        # sample a slice from user u randomly. \n",
    "        idx = np.random.randint(1, len(data[u]))\n",
    "        start = 0 if idx >= max_len else max_len - idx\n",
    "        len_of_item = max_len - start\n",
    "\n",
    "        seq = np.zeros([max_len], dtype=np.int32)\n",
    "        seq[start:] = data[u][idx-len_of_item:idx]\n",
    "\n",
    "\n",
    "        pos = data[u][idx]\n",
    "        neg = np.zeros([neg_size], dtype=np.int32)\n",
    "\n",
    "\n",
    "        if neg_method == 'rand':\n",
    "            neg = random_neg([pos], n_items, neg_size)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "        return (user, seq, pos, neg)\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    while True:\n",
    "        one_batch = []\n",
    "        for i in range(batch_size):\n",
    "            one_batch.append(sample())\n",
    "        result_queue.put(list(zip(*one_batch)))\n",
    "\n",
    "class Sampler(object):\n",
    "    def __init__(self, data, n_items, n_users, batch_size=128, max_len=20, neg_size=10, n_workers=10, neg_method='rand'):\n",
    "        self.result_queue = Queue(maxsize=int(2e5))\n",
    "        self.processors = []\n",
    "        for i in range(n_workers):\n",
    "            self.processors.append(\n",
    "                Process(target=sample_function, args=(data,\n",
    "                                                    n_items, \n",
    "                                                    n_users,\n",
    "                                                    batch_size, \n",
    "                                                    max_len, \n",
    "                                                    neg_size, \n",
    "                                                    self.result_queue, \n",
    "                                                    np.random.randint(2e9),\n",
    "                                                    neg_method)))\n",
    "            self.processors[-1].daemon = True\n",
    "            self.processors[-1].start()\n",
    "    \n",
    "    def next_batch(self):\n",
    "        return self.result_queue.get()\n",
    "\n",
    "    def close(self):\n",
    "        for p in self.processors:\n",
    "            p.terminate()\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "def log2(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(2, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "class FOSSIL(object):\n",
    "    def __init__(self, args, n_items, n_users):\n",
    "        self.args = args\n",
    "        self.n_items = n_items\n",
    "        self.n_users = n_users\n",
    "        self._build()\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def _build(self):\n",
    "        self.inp = tf.placeholder(tf.int32, shape=(None, None), name='inp') # if maxlen is 5, valid len of sample i is 3, then inp[i] = [0, 0, x, x, x]\n",
    "        self.user = tf.placeholder(tf.int32, shape=(None), name='user')\n",
    "        self.pos = tf.placeholder(tf.int32, shape=(None), name='pos')\n",
    "        self.neg = tf.placeholder(tf.int32, shape=(None, self.args.neg_size), name='neg')  \n",
    "\n",
    "        self.lr = tf.placeholder(tf.float32, shape=None, name='lr')\n",
    "\n",
    "        self.item_embedding1 = tf.get_variable('item_embedding1', \n",
    "                                               shape=(self.n_items+1, self.args.emsize),\n",
    "                                               dtype=tf.float32,\n",
    "                                               regularizer=tf.contrib.layers.l2_regularizer(self.args.l2_reg),\n",
    "                                               initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        self.item_embedding2 = tf.get_variable('item_embedding2', \n",
    "                                               shape=(self.n_items+1, self.args.emsize),\n",
    "                                               dtype=tf.float32,\n",
    "                                               regularizer=tf.contrib.layers.l2_regularizer(self.args.l2_reg),\n",
    "                                               initializer=tf.truncated_normal_initializer(stddev=0.01))     \n",
    "        self.user_bias = tf.get_variable('user_bias', \n",
    "                                               shape=(self.n_users+1, self.args.order),\n",
    "                                               dtype=tf.float32,\n",
    "                                               regularizer=tf.contrib.layers.l2_regularizer(self.args.l2_reg),\n",
    "                                               initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        self.global_bias = tf.get_variable('global_bias', \n",
    "                                               shape=(self.args.order),\n",
    "                                               dtype=tf.float32,\n",
    "                                               regularizer=tf.contrib.layers.l2_regularizer(self.args.l2_reg),\n",
    "                                               initializer=tf.constant_initializer(0.))\n",
    "        self.item_bias = tf.get_variable('item_bias', \n",
    "                                               shape=(self.n_items+1),\n",
    "                                               dtype=tf.float32,\n",
    "                                               regularizer=tf.contrib.layers.l2_regularizer(self.args.l2_reg),\n",
    "                                               initializer=tf.constant_initializer(0.))        \n",
    "\n",
    "        mask_inp = tf.expand_dims(tf.to_float(tf.not_equal(self.inp, 0)), -1) #(batch, maxlen, 1)\n",
    "        len_inp = tf.reduce_sum(tf.squeeze(mask_inp, axis=2), axis=1) #(batch)\n",
    "        item_embed = tf.nn.embedding_lookup(self.item_embedding1, self.inp) * mask_inp #(batch, maxlen, k)\n",
    "        long_term = tf.reduce_sum(item_embed, axis=1) #(batch, k)\n",
    "        long_term = tf.expand_dims(tf.pow(len_inp, -self.args.alpha), -1) * long_term #(batch, k)\n",
    "\n",
    "        effective_order = tf.minimum(len_inp, self.args.order) #(batch)\n",
    "        effective_order = tf.expand_dims(tf.to_float(tf.sequence_mask(effective_order,self.args.order)), -1) #(batch, order, 1)\n",
    "\n",
    "        short_term = tf.nn.embedding_lookup(self.user_bias, self.user) #(batch, order)\n",
    "        short_term = tf.expand_dims(short_term + self.global_bias, axis=-1) #(batch, order, 1)\n",
    "        short_term = short_term * item_embed[:, :-1-self.args.order:-1]  #(batch, order, k)\n",
    "        short_term = tf.reduce_sum(short_term * effective_order, axis=1) #(batch, k)\n",
    "\n",
    "        ### for train only\n",
    "        pos_bias = tf.nn.embedding_lookup(self.item_bias, self.pos) #(batch)\n",
    "        pos_embed = tf.nn.embedding_lookup(self.item_embedding2, self.pos) #(batch, k)\n",
    "        neg_bias = tf.nn.embedding_lookup(self.item_bias, self.neg) #(batch, neg_size)\n",
    "        neg_embed = tf.nn.embedding_lookup(self.item_embedding2, self.neg) #(batch, neg_size, k)\n",
    "\n",
    "        temp_vec = short_term + long_term #(batch, k)\n",
    "\n",
    "        pos_score = pos_bias + tf.reduce_sum(temp_vec*pos_embed, axis=1) #(batch)\n",
    "        neg_score = neg_bias + tf.reduce_sum(tf.expand_dims(temp_vec, axis=1) * neg_embed, axis=2) #(batch, neg_size)\n",
    "        neg_score = tf.reduce_mean(neg_score, axis=1) #(batch)\n",
    "\n",
    "        loss = -tf.reduce_mean(tf.log(tf.clip_by_value(tf.sigmoid(pos_score-neg_score), 1e-24, 1-1e-24)))\n",
    "\n",
    "        ### for prediction only\n",
    "        full_score = self.item_bias + tf.matmul(temp_vec, self.item_embedding2, transpose_b=True) #(batch, n_items+1)\n",
    "        self.prediction = full_score\n",
    "        self.loss = loss\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        self.loss += sum(reg_losses)\n",
    "        if self.args.optim == 'adam':\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        elif self.args.optim == 'sgd':\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        self.recall_at_k, self.ndcg_at_k = self._metric_at_k()\n",
    "\n",
    "    def _metric_at_k(self, k=20):\n",
    "        prediction = self.prediction #(batch, n_items+1)\n",
    "        prediction_transposed = tf.transpose(prediction)\n",
    "        labels = tf.reshape(self.pos, shape=(-1,))\n",
    "        pred_values = tf.expand_dims(tf.diag_part(tf.nn.embedding_lookup(prediction_transposed, labels)), -1)\n",
    "        tile_pred_values = tf.tile(pred_values, [1, self.n_items])\n",
    "        ranks = tf.reduce_sum(tf.cast(prediction[:,1:] > tile_pred_values, dtype=tf.float32), -1) + 1\n",
    "        ndcg = 1. / (log2(1.0 + ranks))\n",
    "        hit_at_k = tf.nn.in_top_k(prediction, labels, k=k)\n",
    "        hit_at_k = tf.cast(hit_at_k, dtype=tf.float32)\n",
    "        ndcg_at_k = ndcg * hit_at_k\n",
    "        return tf.reduce_sum(hit_at_k), tf.reduce_sum(ndcg_at_k)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FPMC(object):\n",
    "    def __init__(self, args, n_items, n_users):\n",
    "        self.args = args\n",
    "        self.n_items = n_items\n",
    "        self.n_users = n_users\n",
    "        self._build()\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def _build(self):\n",
    "        self.inp = tf.placeholder(tf.int32, shape=(None, None), name='inp') # if maxlen is 5, valid len of sample i is 3, then inp[i] = [0, 0, x, x, x]\n",
    "        self.user = tf.placeholder(tf.int32, shape=(None), name='user')\n",
    "        self.pos = tf.placeholder(tf.int32, shape=(None), name='pos')\n",
    "        self.neg = tf.placeholder(tf.int32, shape=(None, self.args.neg_size), name='neg')  \n",
    "\n",
    "        self.lr = tf.placeholder(tf.float32, shape=None, name='lr')\n",
    "\n",
    "\n",
    "        self.VUI = tf.get_variable('user_item', \n",
    "                                               shape=(self.n_users+1, self.args.emsize),\n",
    "                                               dtype=tf.float32,\n",
    "                                               regularizer=tf.contrib.layers.l2_regularizer(self.args.l2_reg),\n",
    "                                               initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        self.VIU = tf.get_variable('item_user', \n",
    "                                               shape=(self.n_items+1, self.args.emsize),\n",
    "                                               dtype=tf.float32,\n",
    "                                               regularizer=tf.contrib.layers.l2_regularizer(self.args.l2_reg),\n",
    "                                               initializer=tf.truncated_normal_initializer(stddev=0.01))     \n",
    "        self.VIL = tf.get_variable('item_prev', \n",
    "                                               shape=(self.n_items+1, self.args.emsize),\n",
    "                                               dtype=tf.float32,\n",
    "                                               regularizer=tf.contrib.layers.l2_regularizer(self.args.l2_reg),\n",
    "                                               initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        self.VLI = tf.get_variable('prev_item', \n",
    "                                               shape=(self.n_items+1, self.args.emsize),\n",
    "                                               dtype=tf.float32,\n",
    "                                               regularizer=tf.contrib.layers.l2_regularizer(self.args.l2_reg),\n",
    "                                               initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "\n",
    "\n",
    "        self.prev = self.inp[:, -1] #(batch)\n",
    "        u = tf.nn.embedding_lookup(self.VUI, self.user) #(batch, k)\n",
    "        prev = tf.nn.embedding_lookup(self.VLI, self.prev) #(batch, k)     \n",
    "\n",
    "        ### for train only\n",
    "        pos_iu = tf.nn.embedding_lookup(self.VIU, self.pos) #(batch, k)\n",
    "        pos_il = tf.nn.embedding_lookup(self.VIL, self.pos) #(batch, k)\n",
    "        pos_score = tf.reduce_sum(u*pos_iu, axis=1) + tf.reduce_sum(prev*pos_il, axis=1) #(batch)\n",
    "\n",
    "        neg_iu = tf.nn.embedding_lookup(self.VIU, self.neg) #(batch, neg, k)\n",
    "        neg_il = tf.nn.embedding_lookup(self.VIL, self.neg) #(batch, neg, k)\n",
    "        neg_score = tf.reduce_sum(tf.expand_dims(u, 1)*neg_iu, axis=2) + tf.reduce_sum(tf.expand_dims(prev, 1)*neg_il, axis=2) #(batch, neg)\n",
    "        neg_score = tf.reduce_mean(neg_score, axis=1) #(batch)\n",
    "\n",
    "        loss = -tf.reduce_mean(tf.log(tf.clip_by_value(tf.sigmoid(pos_score-neg_score), 1e-24, 1-1e-24)))\n",
    "\n",
    "        ### for prediction only\n",
    "        full_score = tf.matmul(u, self.VIU, transpose_b=True) + tf.matmul(prev, self.VIL, transpose_b=True)  #(batch, n_items+1)\n",
    "\n",
    "        self.prediction = full_score\n",
    "        self.loss = loss\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        self.loss += sum(reg_losses)\n",
    "        if self.args.optim == 'adam':\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        elif self.args.optim == 'sgd':\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        self.recall_at_k, self.ndcg_at_k = self._metric_at_k()\n",
    "\n",
    "\n",
    "    def _metric_at_k(self, k=20):\n",
    "        prediction = self.prediction #(batch, n_items+1)\n",
    "        prediction_transposed = tf.transpose(prediction)\n",
    "        labels = tf.reshape(self.pos, shape=(-1,))\n",
    "        pred_values = tf.expand_dims(tf.diag_part(tf.nn.embedding_lookup(prediction_transposed, labels)), -1)\n",
    "        tile_pred_values = tf.tile(pred_values, [1, self.n_items])\n",
    "        ranks = tf.reduce_sum(tf.cast(prediction[:,1:] > tile_pred_values, dtype=tf.float32), -1) + 1\n",
    "        ndcg = 1. / (log2(1.0 + ranks))\n",
    "        hit_at_k = tf.nn.in_top_k(prediction, labels, k=k)\n",
    "        hit_at_k = tf.cast(hit_at_k, dtype=tf.float32)\n",
    "        ndcg_at_k = ndcg * hit_at_k\n",
    "        return tf.reduce_sum(hit_at_k), tf.reduce_sum(ndcg_at_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py continue:\n",
    "\n",
    "train_sampler = Sampler(\n",
    "                    data=train_data, \n",
    "                    n_items=n_items, \n",
    "                    n_users=n_users,\n",
    "                    batch_size=args.batch_size, \n",
    "                    max_len=args.seq_len,\n",
    "                    neg_size=args.neg_size,\n",
    "                    n_workers=args.worker,\n",
    "                    neg_method='rand')\n",
    "\n",
    "val_data = prepare_eval_test(val_data, batch_size=100, max_test_len= 20)\n",
    "test_data = prepare_eval_test(test_data, batch_size=100, max_test_len= 20)\n",
    "\n",
    "\n",
    "checkpoint_dir = '_'.join(['save', args.data, args.model, str(args.lr), str(args.l2_reg), str(args.emsize)])\n",
    "\n",
    "print(args)\n",
    "print ('#Item: ', n_items)\n",
    "print ('#User: ', n_users)\n",
    "\n",
    "model_dict = {'fossil': FOSSIL, 'fpmc': FPMC}\n",
    "assert args.model in ['fossil', 'fpmc']\n",
    "\n",
    "\n",
    "model = model_dict[args.model](args, n_items, n_users)\n",
    "\n",
    "lr = args.lr\n",
    "\n",
    "def evaluate(source, sess):\n",
    "    total_recall = 0.0\n",
    "    total_ndcg = 0.0\n",
    "    count = 0.0\n",
    "    for batch in source:\n",
    "        feed_dict = {model.inp: batch[1], model.user:batch[0], model.pos:batch[2]}\n",
    "        recall, ndcg = sess.run([model.recall_at_k, model.ndcg_at_k], feed_dict=feed_dict)\n",
    "        count += len(batch[0])\n",
    "        total_recall += recall\n",
    "        total_ndcg += ndcg\n",
    "\n",
    "    val_recall = total_recall / count \n",
    "    val_ndcg = total_ndcg / count\n",
    "\n",
    "    return [val_recall, val_ndcg]\n",
    "\n",
    "def main():\n",
    "    global lr\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    all_val_recall = [-1]\n",
    "    early_stop_cn = 0\n",
    "    step_count = 0\n",
    "    train_loss_l = 0.\n",
    "    start_time = time.time()\n",
    "    print('Start training...')\n",
    "    try:\n",
    "        while True:\n",
    "            cur_batch = train_sampler.next_batch()\n",
    "            inp = np.array(cur_batch[1])\n",
    "            feed_dict = {model.inp: inp, model.lr: lr}\n",
    "            feed_dict[model.pos] = np.array(cur_batch[2])\n",
    "            feed_dict[model.neg] = np.array(cur_batch[3])\n",
    "            feed_dict[model.user] = np.array(cur_batch[0])\n",
    "\n",
    "            _, train_loss = sess.run([model.train_op, model.loss], feed_dict=feed_dict)\n",
    "            train_loss_l += train_loss\n",
    "            step_count += 1\n",
    "\n",
    "            if step_count % args.log_interval == 0:\n",
    "                cur_loss = train_loss_l / args.log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| Totol step {:10d} | lr {:02.5f} | ms/batch {:5.2f} | loss {:5.3f}'.format(\n",
    "                        step_count, lr, elapsed * 1000 / args.log_interval, cur_loss))\n",
    "                sys.stdout.flush()\n",
    "                train_loss_l = 0.\n",
    "                start_time = time.time()\n",
    "\n",
    "            if step_count % args.eval_interval == 0 and step_count > args.warm_up:\n",
    "                val_recall, val_ndcg = evaluate(val_data, sess)\n",
    "                all_val_recall.append(val_recall)\n",
    "                print('-' * 90)\n",
    "                print('| End of step {:10d} | valid recall@20 {:8.5f} | valid ndcg@20 {:8.5f}'.format(\n",
    "                        step_count, val_recall, val_ndcg))\n",
    "                print('=' * 90)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                if all_val_recall[-1] <= all_val_recall[-2]:\n",
    "                    lr = lr * args.lr_decay\n",
    "                    lr = max(lr, 1e-6)\n",
    "                    early_stop_cn += 1\n",
    "                else:\n",
    "                    early_stop_cn = 0\n",
    "                    model.saver.save(sess, checkpoint_dir + '/model.ckpt')\n",
    "                if early_stop_cn == 3:\n",
    "                    print('Validation recall decreases in three consecutive epochs. Stop Training!')\n",
    "                    sys.stdout.flush()\n",
    "                    break\n",
    "                start_time = time.time()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        train_sampler.close()\n",
    "        exit(1)\n",
    "    train_sampler.close()\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        model.saver.restore(sess, '{}/{}'.format(checkpoint_dir, 'model.ckpt'))\n",
    "        print('Restore model successfully')\n",
    "    else:\n",
    "        print('Restore model failed!!!!!')\n",
    "    test_recall, test_ndcg = evaluate(test_data, sess)        \n",
    "    print('-' * 90)\n",
    "    print('test recall@20 {:8.5f} | test ndcg@20 {:8.5f}'.format(\n",
    "            test_recall, test_ndcg))\n",
    "    print('=' * 90)         \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
